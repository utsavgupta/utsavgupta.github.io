[
    
        
            {
                "ref": "https://utsavgupta.in/blog/leader-election/",
                "title": "Leader election",
                "section": "post",
                "date" : "2020.11.11",
                "body": "Photo by Markus Spiske on Unsplash Recently all our services were replicated to multiple data centers on Google Cloud Platform. The REST API services scaled well without any code changes. However the batch jobs and their schedulers needed attention to ensure we avoid any kind of race conditions.\nLeader election is ubiquitous in today\u0026rsquo;s world of distributed computing. Knowingly or unknowingly we use a multitude of tools that make use of leader election to ensure systems run like clockwork. Cassandra, Hadoop, and Spark are few examples.\nIn this article will explore the idea, and one of its implementations in Go using Google Datastore for persisting data.\nWhat is leader election ? Before I answer that question, we need to understand how modern web applications and services are deployed. Much of the deployment strategy is decided by non functional requirements of the project. Requirements such as minimum availability and response times are taken into consideration for deciding the amount of redundancy and the location of the deployments. For example, if we want to make a service highly available, we would need to make replicas of it. So even if an instance dies, there would always be other instances available to carry out the same duties.\nMaking deployments redundant present us with another problem of ensuring that no two instances of the same job are performing the same piece of work at the same time. That of course does not imply that two instances cannot run at the same time, they shouldn\u0026rsquo;t just get in each others way that will result in the system becoming inconsistent.\nEnter leader election.\nIt is the process of electing one of the replicas as the leader and letting it decide what to do, while the other replicas either remain idle or follow the leader. The decision depends on your use case. The leader may choose to perform a piece of task on its own, or it may decide to split the task into smaller chunks and distribute them to the other replicas.\nHow can this be implemented ? There are quite a few tools and techniques available for implementing leader election in your project. The one we explore here is based on an idea presented in this video (double thumbs up üëçüèΩüëçüèΩ to the folks at Microsoft for creating amazing documentation and training material). I quite liked this approach for its simplicity and the fact that it can work with any database system that features atomic operations.\nThe job As a contrived example, let us create a job that preaches how cool Go is. Every time the job is scheduled it says something preachy about the language. However, in case we decided to scale up our preachers, we\u0026rsquo;d like to ensure that we are not getting too preachy. That is to say that at any given point in time only one job gets to preach the message.\nOur preacher is simple. It accepts a StringWriter as an argument and it preaches by writing the message to the string writer. Moreover since it is a preacher it likes to repeat the message, for which it accepts an integer indicating how many times the message should be printed.\nWe create a custom type, Job, which consists of a name and a doable (a function). In the next section we will create a scheduler which accepts a job that gets scheduled periodically.\ntype Job struct { Name string Do func(int) } func NewPreacher(name, writer io.StringWriter) Job { return Job{ Name: name, Do: func(times int) { // Printing the same message without any pause may alienate my audience.  // Thus I sleep for 1 second between each preaching.  // Don\u0026#39;t sleep before the first preaching \tsleep := false for i := 1; i \u0026lt;= times; i++ { if sleep { time.Sleep(1 * time.Second) } _, err := writer.WriteString(\u0026#34;Go is the best modern programming language\\n\u0026#34;) if err != nil { logger.Error(err) } sleep = true } }, } } The scheduler At the heart of our application is the scheduler. It is this component that on getting elected successfully schedules the job.\nBefore implementing the scheduling logic we need to create a data structure that would hold the lease information. In our case we can define a lease as a temporary right to schedule a job. It is important to understand that these leases need to be temporary to avoid a situation where a scheduler becomes the leader by getting the lease but eventually crashes. This would result in a deadlock wherein the previous leader is not available to work, yet other schedulers cannot become the new leader.\nA lease can be uniquely identified by its job name, as there can be exactly one scheduler as the leader for a job. Thus, we can use the job name as a name key, and include only the leader name and expiry in the structure.\ntype lease struct { Leader string Expiry time.Time } While creating a scheduler we need to assign it a name to be able to uniquely identify it in the election process.\ntype Scheduler func(jobs.Job, time.Duration) func NewScheduler(nodeName string, client *datastore.Client) Scheduler { return func(ctx context.Context, job jobs.Job, t time.Duration) { for { time.Sleep(t) if becomeLeader(ctx, nodeName, job.Name, client, t) { job.Do(3) // the preacher prints the message thrice \t} } } } Finally we need to implement the becomeLeader function.\nA scheduler can gain leadership in one of the following three scenarios\n There is no leader yet The scheduler is already the leader The lease of the previous leader has expired  Reading and writing of lease entities need to be done in a transaction to ensure we avoid any race conditions. To guarantee atomicity of our database operations we make use of the client\u0026rsquo;s RunInTransaction function.\nfunc becomeLeader(ctx context.Context, nodeName, jobName string, client *datastore.Client, t time.Duration) bool { _, err := client.RunInTransaction(ctx, func(tx *datastore.Transaction) error { var l lease key := datastore.NameKey(\u0026#34;Lease\u0026#34;, jobName, nil) err := tx.Get(key, \u0026amp;l) if err != nil \u0026amp;\u0026amp; err != datastore.ErrNoSuchEntity { return err } // Become the leader only if an entry for the given job does not exist \t// OR the lease of the previous leader has already expired \t// OR the current scheduler was the previous leader \tif err == datastore.ErrNoSuchEntity || l.Expiry.Before(time.Now()) || l.Leader == nodeName { l.Leader = nodeName l.Expiry = time.Now().Add(t) _, err := tx.Put(key, \u0026amp;l) return err } return fmt.Errorf(\u0026#34;Node %s could not become leader for job %s\u0026#34;, nodeName, jobName) }) return err == nil } Putting the components together ctx := context.Background() client, e := datastore.NewClient(ctx, \u0026#34;\u0026#34;) if e != nil { panic(e) } scheduler := NewScheduler(\u0026#34;karmic-koala\u0026#34;, client) job := NewPreacher(\u0026#34;go_preacher\u0026#34;, os.Stdout) scheduler(ctx, job, 1 * time.Minute) // schedule the job every minute You are encouraged to play around with the working example available at https://github.com/utsavgupta/go-leader-election.\n"
            }
        
    ,
        
            {
                "ref": "https://utsavgupta.in/blog/installing-multinode-kubernetes-cluster-kubeadm/",
                "title": "Installing a multinode Kubernetes cluster using kubeadm",
                "section": "post",
                "date" : "2020.07.29",
                "body": "I have been playing around with Kubernetes in various settings for almost three years. Majority of it in production. Where the infrastructure was either setup and configured by experienced platform engineers, or it was hosted on Google as GKE clusters. My interaction with Kubernetes has been limited to mostly handling the deployments that I work on.\nRecently I decided to start learning Kubernetes at a greater depth. So what better place to start than installing and configuring a multi node cluster on my laptop? To be honest, it took me a few attempts to get things working. For that reason I decided to create this blog post documenting the process as a tutorial. If you too are curious about setting up a Kubernetes playground on your laptop or PC, read along.\nBefore we begin For this tutorial we are going to install the Kubernetes nodes on VirtualBox virtual machines running Debian Buster. The steps would be slightly different for any other combination of hypervisior and guest operating system.\nIt is recommended that your system has sufficient memory and CPUs available that can be allocated to the virtual machines. The test cluster should work fine on a computer with a modest Intel Core i5 or AMD Ryzen processor as long as the host has enough free memory to share with the VMs.\nCreating the VMs and setting up the network At the end of this tutorial we would like to have a Kubernetes cluster with a single control plane and two workers. That translates to three virtual machines. As for network interfaces: we would want to create a couple of them. First, a NAT adapter for the virtual machines to access the Internet. Second, a Host-Only interface for letting the the VMs communicate amongst each other and the host operating system. The following diagram gives an overview of how we plan to implement our network.\nBefore you begin make sure that the VBoxManage is accessible from your shell. In case it isn\u0026rsquo;t, update the PATH variable to include the path to your VirtualBox installation.\nLet\u0026rsquo;s start with creating the virtual machines.\nPS\u0026gt; VBoxManage createvm --name k8smaster --ostype Linux_64 --register PS\u0026gt; VBoxManage createvm --name k8snode1 --ostype Linux_64 --register PS\u0026gt; VBoxManage createvm --name k8snode2 --ostype Linux_64 --register Each worker node will be allocated 1 cpu, 2 GB memory, and 12 MB video memory. The master will get 2 cpus and the other allocations remain unchanged.\nPS\u0026gt; VBoxManage modifyvm k8smaster --cpus 2 --memory 2048 --vram 12 PS\u0026gt; VBoxManage modifyvm k8snode1 --cpus 1 --memory 2048 --vram 12 PS\u0026gt; VBoxManage modifyvm k8snode2 --cpus 1 --memory 2048 --vram 12 Next we need to create virtual disks that will be used for secondary storage by our virtual machines. We wil create three dynamic storage disks 10 GB each.\nPS\u0026gt; VBoxManage createhd --filename \u0026#39;D:\\Virtual Machines\\k8smaster.vdi\u0026#39; --size 10240 --variant Standard PS\u0026gt; VBoxManage createhd --filename \u0026#39;D:\\Virtual Machines\\k8snode1.vdi\u0026#39; --size 10240 --variant Standard PS\u0026gt; VBoxManage createhd --filename \u0026#39;D:\\Virtual Machines\\k8snode2.vdi\u0026#39; --size 10240 --variant Standard Before proceeding make sure to download the Debain installation image from here.\nOnce you have downloaded the image on system, we can go ahead and configure the virtual machines to use the Debian disk image and the newly created virtual hard disk.\n# configure the master vm PS\u0026gt; VBoxManage storagectl k8smaster --name \u0026#34;SATA Controller\u0026#34; --add sata --bootable on PS\u0026gt; VBoxManage storagectl k8smaster --name \u0026#34;IDE Controller\u0026#34; --add ide PS\u0026gt; VBoxManage storageattach k8smaster --storagectl \u0026#34;SATA Controller\u0026#34; --port 0 --device 0 --type hdd --medium \u0026#34;D:\\Virtual Machines\\k8smaster.vdi\u0026#34; PS\u0026gt; VBoxManage storageattach k8smaster --storagectl \u0026#34;IDE Controller\u0026#34; --port 0 --device 0 --type dvddrive --medium \u0026#34;D:\\Downloads\\debian-10.4.0-amd64-netinst.iso\u0026#34; # configure node1 vm PS\u0026gt; VBoxManage storagectl k8snode1 --name \u0026#34;SATA Controller\u0026#34; --add sata --bootable on PS\u0026gt; VBoxManage storagectl k8snode1 --name \u0026#34;IDE Controller\u0026#34; --add ide PS\u0026gt; VBoxManage storageattach k8snode1 --storagectl \u0026#34;SATA Controller\u0026#34; --port 0 --device 0 --type hdd --medium \u0026#34;D:\\Virtual Machines\\k8snode1.vdi\u0026#34; PS\u0026gt; VBoxManage storageattach k8snode1 --storagectl \u0026#34;IDE Controller\u0026#34; --port 0 --device 0 --type dvddrive --medium \u0026#34;D:\\Downloads\\debian-10.4.0-amd64-netinst.iso\u0026#34; # configure node2 vm PS\u0026gt; VBoxManage storagectl k8snode2 --name \u0026#34;SATA Controller\u0026#34; --add sata --bootable on PS\u0026gt; VBoxManage storagectl k8snode2 --name \u0026#34;IDE Controller\u0026#34; --add ide PS\u0026gt; VBoxManage storageattach k8snode2 --storagectl \u0026#34;SATA Controller\u0026#34; --port 0 --device 0 --type hdd --medium \u0026#34;D:\\Virtual Machines\\k8snode2.vdi\u0026#34; PS\u0026gt; VBoxManage storageattach k8snode2 --storagectl \u0026#34;IDE Controller\u0026#34; --port 0 --device 0 --type dvddrive --medium \u0026#34;D:\\Downloads\\debian-10.4.0-amd64-netinst.iso\u0026#34; That leaves us only with the network configuration. When you create a new virtual machine on VirtualBox, by default nic1 is configured to use a NAT adapter. You can verify this by running VBoxManage showvminfo \u0026lt;vmname\u0026gt; | grep \u0026quot;NIC 1\u0026quot;.\nThe last step in our setup is to configure the host only interface for internal communication. Before we do so, we need to decide the ip ranges for the network. We can use the IP range 10.10.0.1/16 on this network interface. The master and the two worker vms can be assigned IPs 10.10.0.2, 10.10.0.3, and 10.10.0.4 respectively. We also need to define a subnet for our pods. For which we can use 10.10.128.1/17. This subnet will be required later when we configure the CNI.\nPS\u0026gt; VBoxManage hostonlyif create ... Interface \u0026#39;VirtualBox Host-Only Ethernet Adapter #5\u0026#39; was successfully created ... PS\u0026gt; VBoxManage hostonlyif ipconfig \u0026#34;VirtualBox Host-Only Ethernet Adapter #5\u0026#34; --ip 10.10.0.1 --netmask 255.255.0.0 # configure the vms to use the nic PS\u0026gt; VBoxManage modifyvm k8smaster --nic2 hostonly --hostonlyadapter1 \u0026#34;VirtualBox Host-Only Ethernet Adapter #5\u0026#34; PS\u0026gt; VBoxManage modifyvm k8snode1 --nic2 hostonly --hostonlyadapter1 \u0026#34;VirtualBox Host-Only Ethernet Adapter #5\u0026#34; PS\u0026gt; VBoxManage modifyvm k8snode2 --nic2 hostonly --hostonlyadapter1 \u0026#34;VirtualBox Host-Only Ethernet Adapter #5\u0026#34; Installing and configuring the operating system We are now ready to install the operating system on our virtual machines. Fire up the vms with the following commands.\nPS\u0026gt; VBoxManage startvm k8smaster PS\u0026gt; VBoxManage startvm k8snode1 PS\u0026gt; VBoxManage startvm k8snode2 Select the second option from the list, Install.\nChoose your preferred language and location settings.\nIn the next screen you\u0026rsquo;ll be asked to select your preferred network interface. Select enp0s3. This will give the installer access to the internet for pulling the missing packages.\nFor the host names, let\u0026rsquo;s assign them k8smaster, k8snode1, and k8snode2 respectively. We can put all the VMs in a domain named kubernetest.\nAfter setting the root password. The installer will ask you to create a new user. We will be using this new user to SSH into the nodes.\nNext you\u0026rsquo;ll be asked to configure your storage medium, and specify a mount point the file system. Either you can select the guided option or you can manually create a partition for the file system. In case you do it manually, do not create a swap partition. This would anyway need to be turned off before we can bootstrap Kubernetes. However, if you select the guided mode don\u0026rsquo;t worry we can turn off swap usage later.\nIn the software selection page, select only SSH server and Standard system utilities. We do not need a desktop environment for any of the virtual machines.\nOnce the installation completes you\u0026rsquo;ll be asked to confirm the master boot record. Select /dev/sda from the list and complete installation.\nConfiguring the operating system Congratulations! You have successfully installed Debian on all three virtual machines. Login to the machines with root credentials to install a few utilities that will be required to complete the setup.\nroot@\u0026lt;vm-hostname\u0026gt;:~$ apt-get update \u0026amp;\u0026amp; apt-get install -y sudo curl gnupg net-tools # add your user to the list of sudoers root@\u0026lt;vm-hostname\u0026gt;:~$ usermod -aG sudo \u0026lt;username\u0026gt; We are left with making the machines acquire IPs on the host only interface. For this we will have to make the following entries to /etc/network/interfaces.\nauto enp0s8 iface enp0s8 inet static address 10.10.0.X # where X=2 for k8smaster, 3 for k8snode1, and 4 for k8snode2 network 10.10.0.1 netmask 255.255.0.0 You can now restart your networking services and verify the changes using ifconfig.\nroot@\u0026lt;vm-hostname\u0026gt;:~$ systemctl restart networking root@\u0026lt;vm-hostname\u0026gt;:~$ ifconfig ... enp0s8: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.0.X netmask 255.255.0.0 broadcast 10.10.255.255 inet6 ... prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether ... txqueuelen 1000 (Ethernet) RX packets 119 bytes 11191 (10.9 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 98 bytes 11925 (11.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ... That completes the setup of the environment. We can now focus on installing Docker and Kubernetes. You can now reboot all three virtual machines and login into them using SSH from your host terminal.\nInstalling Docker and Kubernetes We will be installing both pieces of software from their official repositories.\n# download and add gpg keys \u0026lt;user\u0026gt;@\u0026lt;vm-hostname\u0026gt;~$ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - \u0026lt;user\u0026gt;@\u0026lt;vm-hostname\u0026gt;~$ curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # add repository details to the list of apt sources \u0026lt;user\u0026gt;@\u0026lt;vm-hostname\u0026gt;~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/docker.list deb [arch=amd64] https://download.docker.com/linux/debian buster stable EOF \u0026lt;user\u0026gt;@\u0026lt;vm-hostname\u0026gt;~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/k8s.list deb [arch=amd64] https://apt.kubernetes.io/ kubernetes-xenial main EOF # install docker and kubernetes \u0026lt;user\u0026gt;@\u0026lt;vm-hostname\u0026gt;~$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y docker-ce docker-ce-cli containerd.io kubeadm kubectl kubelet Bootstrapping the cluster Now our virtual machines are ready to be bootstrapped with kubeadm. First, we need to set up the control plane on the k8smaster virtual machine. Then we will make the other two vms join the cluster as workers.\nIn case you have not disabled swap already, you can do it now with the following command.\n\u0026lt;user\u0026gt;@\u0026lt;vm-hostname\u0026gt;~$ sudo swapoff -a We will execute kubeadm init to create the control plane on the k8smaster after which we shall install calico as our CNI.\n\u0026lt;user\u0026gt;@k8smaster~$ sudo kubeadm init --apiserver-advertise-address=10.10.0.2 --pod-network-cidr=10.10.128.1/17 ... To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.10.0.2:6443 --token 8rgorl.4h52rlf8rau2jxq7 \\  --discovery-token-ca-cert-hash sha256:39fa02d151ad3d1ded96956060663ca893ece4cc0e1212b58ae1c417afd30c1d Copy the admin.conf file to your home directory as shown in the output above. Make a note of the token and the certificate hash displayed by the above command. We will need those to make the other virtual machines join the cluster. But before we do that, let\u0026rsquo;s install our CNI.\nWe will download the manifest file from the official website of calico and then make changes to it in order to include our new network settings.\n\u0026lt;user\u0026gt;@k8smaster~$ wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml Modify the value of CALICO_IPV4POOL_CIDR to 10.10.128.1/17. Once done save the file and apply the manifest.\n\u0026lt;user\u0026gt;@k8smaster~$ kubectl apply -f calico.yaml # verify that the calico nodes are up and running \u0026lt;user\u0026gt;@k8smaster~$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-76d4774d89-fn5pr 1/1 Running 0 3m2s calico-node-z8bs5 1/1 Running 0 3m2s coredns-66bff467f8-ckq26 1/1 Running 0 25m coredns-66bff467f8-k8r2b 1/1 Running 0 25m etcd-k8smaster 1/1 Running 0 26m kube-apiserver-k8smaster 1/1 Running 0 26m kube-controller-manager-k8smaster 1/1 Running 0 26m kube-proxy-8bvfz 1/1 Running 0 25m kube-scheduler-k8smaster 1/1 Running 0 26m You can now switch to the other two virtual machines and run the following command to join the cluster. (Turn off swap in the other VMs if you have not done it already)\n# join the cluster from node 1 \u0026lt;user\u0026gt;@k8snode1~$ sudo kubeadm join 10.10.0.2:6443 --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt; # join the cluster from node 2 \u0026lt;user\u0026gt;@k8snode2~$ sudo kubeadm join 10.10.0.2:6443 --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt; Making our first deployment Now we are ready to make our first deployment on our cluster.\n\u0026lt;user\u0026gt;@k8smaster~$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/controllers/nginx-deployment.yaml \u0026lt;user\u0026gt;@k8smaster~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-6b474476c4-7z47l 1/1 Running 0 60s 10.10.181.2 k8snode2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b474476c4-ddgwc 1/1 Running 0 60s 10.10.130.129 k8snode1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b474476c4-r7hn2 1/1 Running 0 60s 10.10.181.1 k8snode2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; And that\u0026rsquo;s it. You have successfully deployed your first application on a Kubernetes cluster that you created from scratch.\nThat concludes the tutorial. I plan to write more blog posts on Kubernetes and other workload orchestration tools as I progress on this journey.\n"
            }
        
    ,
        
            {
                "ref": "https://utsavgupta.in/blog/unison-datastore-migration-go/",
                "title": "Unison: A Datastore migration library for Google Go",
                "section": "post",
                "date" : "2020.03.17",
                "body": "Unison is a lightweight library that allows you to version and manage your Datastore migrations. In this blog article we will have a look at how you can install the Unison command line tool and get started migrating entities to Datastore.\nNote: At the time of writing this article the library was not compatible with Google Firestore.\nBefore we begin, the article assumes that you have Google Go installed on your computer. In case you do not have it installed, click here to grab an installer for your operating system.\nThe go-unison project comprises two packages, viz, unison and unisoner. The first is the library that you will use to migrate your scripts, while the latter is a command line tool that lets you generate new migration scripts.\nLet\u0026rsquo;s start with creating a migration script that migrates some delicious fruits to Datastore. Run the following commands to set up your project.\n$ mkdir goapp $ cd goapp goapp $ go mod init goapp goapp $ go get cloud.google.com/go/datastore goapp $ mkdir ent goapp $ touch ent/fruit.go goapp $ touch main.go The ent/fruit.go file should contain the serializable Datastore entity structure as follows.\npackage ent type Fruit struct { ID string `datastore:\u0026#34;id\u0026#34;` Name string `datastore:\u0026#34;Name\u0026#34;` } Now let\u0026rsquo;s go ahead and install the unisoner command line tool and the unison library.\ngoapp $ go get github.com/utsavgupta/go-unison/unisoner goapp $ go get github.com/utsavgupta/go-unison/unison We are now ready to create new migration files. When you execute the unisoner command it creates the following to bootstrap unison.\n Migration package: A new package is created in the present working directory. By default the name of this package is migration. But this can be overridden by either setting the environment variable unison_migration_package or by passing --migration_package \u0026lt;pkg_name\u0026gt; while executing the command. Note: if values are received from both, environment variable and the command line param, the latter will be given precedence. Unison migrations type: A new type gets created within the above package. It is on this type new migrations will be defined. Finally an instance of this type needs to be passed to the unison library for it to work its magic.  Now with the details out of the way let\u0026rsquo;s create our first migration script.\ngoapp $ unisoner --migration_package gcp Filename [.go extension is automatically appended] (default -\u0026gt; Apply1584396052): fruits Description: add fruits Let\u0026rsquo;s examine the files that were generated by unison.\ngoapp $ ls gcp/ fruits.go unison.go The file unison.go contains the migrations type that has been explained above. The other file is where the migration script goes.\nOpen gcp/fruits.go in a text editor and the contents should look like the following.\npackage gcp import ( \u0026#34;cloud.google.com/go/datastore\u0026#34; ) // Apply1584396052 add fruits func (u *UnisonMigrations) Apply1584396052(t *datastore.Transaction, ns string) error { return nil } Each migration file we generate should ideally contain one method defined on UnisonMigrations. These method names are based on the following naming convention Apply\u0026lt;Timestamp\u0026gt;. It is based on this timestamp that the unison library sorts the order of migrations. A migration once successfully commited will not be run again. Only migrations that have a timestamp greater than the last successully executed migration will be executed.\nSince I love eating apples and mangoes, I will migrate these two fruits to Datastore. Let\u0026rsquo;s make the following changes to gcp/fruits.go (you can use your favorites here üòâ).\npackage gcp import ( \u0026#34;goapp/ent\u0026#34; \u0026#34;cloud.google.com/go/datastore\u0026#34; ) // Apply1584396052 add fruits func (u *UnisonMigrations) Apply1584396052(t *datastore.Transaction, ns string) error { fruits := []ent.Fruit{ ent.Fruit{ID: \u0026#34;apple\u0026#34;, Name: \u0026#34;Apple\u0026#34;}, ent.Fruit{ID: \u0026#34;mango\u0026#34;, Name: \u0026#34;Mango\u0026#34;}, } keys := make([]*datastore.Key, len(fruits)) for idx, fruit := range fruits { keys[idx] = datastore.NameKey(\u0026#34;Fruits\u0026#34;, fruit.ID, nil) keys[idx].Namespace = ns } _, err := t.PutMulti(keys, fruits) return err } Great! Our first migration script is ready. Now all that remains is to use the unison library to migrate this script. For that let\u0026rsquo;s make changes to main.go.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;goapp/gcp\u0026#34; \u0026#34;cloud.google.com/go/datastore\u0026#34; \u0026#34;github.com/utsavgupta/go-unison/unison\u0026#34; ) const ( namespace = \u0026#34;unison-demo\u0026#34; ) func main() { dsClient, err := datastore.NewClient(context.Background(), \u0026#34;*detect-project-id*\u0026#34;) if err != nil { panic(err) } var unisonMigrations gcp.UnisonMigrations unison.RunMigrations(dsClient, namespace, \u0026amp;unisonMigrations) } We are a step away from migrating our first entities to Datastore. Before running the code make sure you have Google application credentials set. More on it here.\nAfter you have set the variable you will be ready to run your shiny new migration script.\ngoapp $ go run . Running Unison Applying migration 1584477480 ... Done We are done !! Yay ! We are done migrating our first migration script. Head over to the Google Cloud console to verify the changes.\nNote that the migration records themselves are stored as entites of kind UnisonMigrationMeta.\nNow with the first migration script done, let\u0026rsquo;s create a new script to migrate a few more fruits.\ngoapp $ unisoner --migration_package gcp Filename [.go extension is automatically appended] (default -\u0026gt; Apply1584126043): more_fruits Description: add more fruits Edit the newly created gcp/more_fruits.go file to migrate a few more fruits.\npackage gcp import ( \u0026#34;goapp/ent\u0026#34; \u0026#34;cloud.google.com/go/datastore\u0026#34; ) // Apply1584126043 add fruits func (u *UnisonMigrations) Apply1584126043(t *datastore.Transaction, ns string) error { fruits := []ent.Fruit{ ent.Fruit{ID: \u0026#34;banana\u0026#34;, Name: \u0026#34;Banana\u0026#34;}, ent.Fruit{ID: \u0026#34;orange\u0026#34;, Name: \u0026#34;Orange\u0026#34;}, ent.Fruit{ID: \u0026#34;watermelon\u0026#34;, Name: \u0026#34;Watermelon\u0026#34;}, } keys := make([]*datastore.Key, len(fruits)) for idx, fruit := range fruits { keys[idx] = datastore.NameKey(\u0026#34;Fruits\u0026#34;, fruit.ID, nil) keys[idx].Namespace = ns } _, err := t.PutMulti(keys, fruits) return err } Running go run . again should migrate only the new migration script.\ngoapp $ go run . Running Unison Applying migration 1584126043 ... Done We are done !! Please note: The project is in it\u0026rsquo;s early stages. Contributions in terms of bug reports, documentation, and testing are welcome. Do not hesitate to report issues or to raise pull requests.\n"
            }
        
    
]