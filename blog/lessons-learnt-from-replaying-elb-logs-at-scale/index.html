<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Lessons learnt from replaying ELB logs at scale</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Fira+Mono&display=swap" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300;700&display=swap" rel="stylesheet"> 
<link href="/css/fontawesome/css/all.min.css" rel="stylesheet">


    
    <link rel="stylesheet" href="https://utsavgupta.in/css/main.css">



    
    <link rel="stylesheet" href="https://utsavgupta.in/css/syntax-light.css">



    
    <link rel="stylesheet" href="https://utsavgupta.in/css/syntax-dark.css">

</head>
<body>
    <header>
        <a class="nav-logo-anchor" href="/"><div class="nav-logo"></div></a>
<div class="box-second-last"><a class="nav-link" href="/about">about</a></div>
<div class="box-last"><a class="nav-link" href="/tags">tags</a></div>
    </header>
    <main>
        
    <section class="main-content">
        <h1>Lessons learnt from replaying ELB logs at scale</h1>
        <div class="post-meta">
            <i class="fa fa-calendar-alt"></i>&nbsp;Mar 13, 2022 | Written By Utsav Gupta
        </div>
        <div class="vspacer-1rem"></div>
        <p>As part of our exercise of porting a Kotlin application to Go, we wanted to test the port by mirroring production traffic to it in lower environments. Our applications are hosted on AWS, and initially the platform team had considered VPC mirroring to accomplish this goal. However, the option turned out to be infeasible for reasons unbeknownst to me.</p>
<p>We resorted to re-playing ELB log messages from one environment to another. This is a fairly common activity and we hoped to find a repository for the same on Github.</p>
<p>We came across the following Go projects for the purpose of playing ELB access logs.</p>
<ul>
<li><a href="https://github.com/ONSdigital/aws-elb-replay">ONSdigital/aws-elb-replay</a></li>
<li><a href="https://github.com/AppsFlyer/elb-log-replay">AppsFlyer/elb-log-replay</a></li>
</ul>
<p>The projects did not completely meet our requirements, but were a good starting point.</p>
<p>The first solution accepts log files from the previous day (the time frame can be configured), loops over the log messages creating a goroutine for each log entry. The goroutines sleep until it is time to send the request. So for example, at 1300 hrs if I process a log file containing a message from 1700 hrs yesterday it will create a goroutine that sleeps for 4 hours before sending the request. This technique works if you have a small number of requests. When we tried running the application with a few thousand log messages, it froze. And that was it. The solution did not seem scalable so we decided to explore the second option.</p>
<p>The latter is eleganlty designed. I urge you check the <a href="https://github.com/AppsFlyer/elb-log-replay/blob/master/README.md">readme file</a> in the repository. The authors have done a great job of documenting the architecture.</p>
<p>The following quote from the read me summarises how the player has been implemented.</p>
<blockquote>
<p>To make this possible we chose a design pattern called pipelines also described here <a href="https://blog.golang.org/pipelines">https://blog.golang.org/pipelines</a></p>
<ul>
<li>A single routine reads the log files and sends them one by one to a channel.</li>
<li>On the other side of this channel there are num-senders routines that consume from that channel and for each log line they: parse the log line and synchronously send the request and wait for the response.</li>
</ul>
<p>Eventually we collect the stats (actual rate and latency) and output them for monitoring puprposes. <em>(sic)</em></p>
</blockquote>
<p>The project provided us a robust foundation to build on. In this post I document five takeaways from working on enhancing AppsFlyer/elb-log-replay (hereafter referred to as the base project).</p>
<h2 id="the-downloader">The downloader</h2>
<p>The base project has been developed as a command line application. It accepts the path to a local directory and then sends each file present in the directory through the pipeline described above.</p>
<p>We needed to design a job that would fetch the log files from Amazon S3, process them (described in the next section), and then pass the list of files to the player.</p>
<p>We decided to strip away the command line interface of the base project and replace it with a shiny new GRPC server. At this point I&rsquo;m sure I&rsquo;ve managed to raise a few eyebrows by bringing GRPC into the design. We wanted to ensure that the downloader and the log player do not get into contention for CPU time. A simple solution was to separate the two into independent processes and specify their CPU allocation with <code>GOMAXPROCS</code>.</p>
<p>The approach allowed us to download log files and process them periodically while maintaining the desired throughput of the log player.</p>
<h2 id="traffic-shaping">Traffic shaping</h2>
<p>Before we could send the log files for replaying we had to weed out access logs of routes that we did not intend to replay. This took away any processing task that might have been needed by the base project.</p>
<p>Our Elastic Load Balancer instances are configured to generate log files at an interval of 5 minutes. As a first step we calculated the throughput using the following formula.</p>
<pre><code>throughput = no_of_lines / ( 5 * 60 )
</code></pre>
<p>The filtered logs and the desired throughput were saved as two independent files and the parent directory was sent to the base project for replaying by the downloader every five minutes via GRPC.</p>
<p>A major limitation of this implementation was that we were not able to reproduce the traffic spikes we were seeing in production. Taking an average of five minutes was smoothing out momentary surges. We decided to split a five minute window into twenty quartiles, that helped us paint a picture closer to reality.</p>
<h2 id="scaling-out">Scaling out</h2>
<p>Having one compute instance simulate the traffic generated by tens or even hundreds of thousands of people is unrealistic. The system was designed keeping this in mind.</p>
<p>In fact, AWS&rsquo;s way of saving a log file per ELB instance meant that all we had to do was to have one compute instance for each ELB instance.</p>
<p>This setup helped us easily achieve throughputs north of 700k requests per minute, with enough cpu and networking resources to spare.</p>
<h2 id="tuning-the-system-for-throughput">Tuning the system for throughput</h2>
<p>Getting the math right on calculating how much load a compute instance was able to generate was important. For example, if the response time of a request is about <code>20ms</code>, we can have a throughput of only 50 requests per second per connection. If we have a connection pool of 5, we increase the throughput five-fold to 250 requests per second.</p>
<p>While configuring the pool size we encountered two gotchas.</p>
<ul>
<li>
<p>Each connection in the pool is represented by a file descriptor. While creating a pool of connections you are limited by the number of open file descriptors you are allowed to have. You can use <code>uname -n</code> to check and also set the limit.</p>
</li>
<li>
<p>This gotcha is specific to Go. Even though you can set the connection pool size using the <code>MaxIdleConns</code> property, you will be limited to having only 2 connections per host unless you override the default value by setting <code>MaxIdleConnsPerHost</code>.</p>
</li>
</ul>
<p>A big shout out to <a href="http://tleyden.github.io/blog/2016/11/21/tuning-the-go-http-client-library-for-load-testing/">tleyden.github.io</a> for documenting their findings in the blog post.</p>
<h2 id="deployment">Deployment</h2>
<p>Given that this tool would be seldom used, and possibly shelved once our transition from Kotlin to Go is complete meant we needed something simpler than having someone from the platform team configure a brand new pipeline for it.</p>
<p>For the first few runs we used the AWS CLI to provision resources, and then scp-ed the binaries and SSH-ed into the compute instances to start the applications. It was an onerous task.</p>
<p>Our first attempt at automating this was to write an elaborate bash script which sooned turned ugly. <a href="https://www.terraform.io/">Terraform</a> provided a great way of provisioning and draining infrastructure resources, as well as deploying and executing our binaries.</p>

    </section>
    <div id="progress-bar"></div>

    </main>
    <footer>
        <b>Utsav Gupta</b> Â© 2022 Powered By <b>Hugo</b>. Hosted on <b>Github Pages</b>. <a href="/privacy" class="emph">Privacy Policy</a>.
    </footer>
    
<script src="/js/app.min.a34612972aec8bdaddec9d49fc714c354091a663f58a96a2d9e1112b98e0345856570c1202e3ba23de852276b5d7fd01b5b575ac2e1356d95453bd466a09c69e.js" integrity="sha512-o0YSlyrsi9rd7J1J/HFMNUCRpmP1ipai2eERK5jgNFhWVwwSAuO6I96FIna11/0BtbV1rC4TVtlUU71GagnGng=="></script>

</body>
</html>